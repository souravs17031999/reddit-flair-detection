{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK - MIDAS@IIITD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART - I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMEEJkycDwUO"
   },
   "source": [
    "# Scraping Reddit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA COLLECTION PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THERE ARE THREE PROCESSES THROUGH WHICH I HAVE SHOWN TO FETCH REDDIT DATA : \n",
    "#### * To use simply request library and raw fectching using automation of reddit authentication (METHOD1)\n",
    "#### * To use PRAW API wrapper (METHOD2)\n",
    "#### * To use PushShift API wrapper (METHOD3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vx-rL_v4DwUV"
   },
   "outputs": [],
   "source": [
    "# import all packages\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from os import path\n",
    "import urllib.request as ulib\n",
    "from urllib.request import Request, urlopen\n",
    "from uuid import uuid4\n",
    "import requests.auth\n",
    "import urllib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJeSjcjdDwUa"
   },
   "outputs": [],
   "source": [
    "# for automating the browser for reddit authentication method 1\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOKpzfrhDwUP"
   },
   "source": [
    "## METHOD 1 : Using Requests to fetch using Reddit API - OAuth Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4w_SVmvDwUe"
   },
   "outputs": [],
   "source": [
    "class RedditAPI:\n",
    "    \"\"\" \n",
    "    Class for implementing METHOD 1\n",
    "      \n",
    "    Attributes: \n",
    "        CLIENT_ID (str): client id taken from reddit app \n",
    "        CLIENT_SECRET (str): client secret taken from reddit app \n",
    "        REDIRECT_URI (str): client redirect url taken from reddit app \n",
    "        \n",
    "        * All things taken from dev account : https://www.reddit.com/prefs/apps/\n",
    "    \"\"\"\n",
    "    def __init__(self, CLIENT_ID, CLIENT_SECRET, REDIRECT_URI):\n",
    "        \"\"\" \n",
    "        Does the initialisation part for reddit requests\n",
    "\n",
    "        Parameters: \n",
    "            CLIENT_ID (str): client id taken from reddit app \n",
    "            CLIENT_SECRET (str): client secret taken from reddit app \n",
    "            REDIRECT_URI (str): client redirect url taken from reddit app \n",
    "\n",
    "        Returns: \n",
    "            None\n",
    "  \n",
    "        \"\"\"\n",
    "        self.CLIENT_ID = CLIENT_ID\n",
    "        self.CLIENT_SECRET = CLIENT_SECRET\n",
    "        self.REDIRECT_URI = REDIRECT_URI\n",
    "    \n",
    "    def make_authorization_url(self):\n",
    "        \"\"\" \n",
    "        helps in creating the complete reddit OAuth url\n",
    "\n",
    "        Parameters: \n",
    "            None\n",
    "\n",
    "        Returns: \n",
    "            url (str) : the reddit url for OAuth authentication.\n",
    "  \n",
    "        \"\"\"\n",
    "        state = str(uuid4())\n",
    "        params = {\"client_id\": self.CLIENT_ID,\n",
    "                  \"response_type\": \"code\",\n",
    "                  \"state\": state,\n",
    "                  \"redirect_uri\": self.REDIRECT_URI,\n",
    "                  \"duration\": \"temporary\",\n",
    "                  \"scope\": \"identity edit flair history modconfig modflair modlog modposts modwiki mysubreddits privatemessages \\\n",
    "                  read report save submit subscribe vote wikiedit wikiread\"}\n",
    "        url = \"https://ssl.reddit.com/api/v1/authorize?\" + urllib.parse.urlencode(params)\n",
    "        return url\n",
    "    \n",
    "    def auth_retreive_code(self):\n",
    "        \"\"\" \n",
    "        Does retrievel of OAuth code\n",
    "\n",
    "        Parameters: \n",
    "            None\n",
    "\n",
    "        Returns: \n",
    "            current_url (str) : two part of current_url , first contains some part of url, second one contains code part of url.\n",
    "  \n",
    "        \"\"\"\n",
    "        auth_url = self.make_authorization_url()\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument('--ignore-certificate-errors')\n",
    "        options.add_argument('--ignore-ssl-errors')\n",
    "        options.add_argument('--disable-notifications')\n",
    "        # initiating driver for chrome\n",
    "        driver = webdriver.Chrome(executable_path='chromedriver.exe', options=options)\n",
    "        try:\n",
    "            driver.get(auth_url)\n",
    "            print(\"Opened webpage..\")\n",
    "            driver.implicitly_wait(random.randint(10, 20))\n",
    "            driver.find_element_by_name(\"username\").send_keys(\"ououmua_2\")\n",
    "            driver.implicitly_wait(random.randint(10, 20))\n",
    "            driver.find_element_by_name(\"password\").send_keys(\"17031999\")\n",
    "            driver.implicitly_wait(random.randint(10, 20))\n",
    "            print(\"Logged in....\")\n",
    "            driver.find_element_by_xpath('/html/body/div/div/div[2]/div/form/div/fieldset[5]/button').click()\n",
    "            driver.implicitly_wait(random.randint(30, 50))\n",
    "            driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/form/div/input[1]').click()\n",
    "            print(\"Got the code.....\")\n",
    "            current_url = driver.current_url\n",
    "            driver.close()\n",
    "            return current_url.split(\"=\")[1], current_url.split(\"=\")[2]\n",
    "        except Exception as error:\n",
    "            print(\"Failing to get code...\")\n",
    "            \n",
    "    def get_token(self):\n",
    "        \"\"\" \n",
    "        Does retrievel of OAuth access tokens\n",
    "\n",
    "        Parameters: \n",
    "            self\n",
    "\n",
    "        Returns: \n",
    "            token_json (str) : access_token needed for authentication to reddit API's\n",
    "  \n",
    "        \"\"\"\n",
    "        state, code = self.auth_retreive_code()\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        client_auth = requests.auth.HTTPBasicAuth(self.CLIENT_ID, self.CLIENT_SECRET)\n",
    "        post_data = {\"grant_type\": \"authorization_code\",\n",
    "                     \"code\": code,\n",
    "                     \"redirect_uri\": self.REDIRECT_URI}\n",
    "        response = requests.post(\"https://ssl.reddit.com/api/v1/access_token\",\n",
    "                                 auth=client_auth,\n",
    "                                 data=post_data, headers=headers)\n",
    "\n",
    "        token_json = response.json()\n",
    "        return token_json['access_token']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "tP41x_bpDwUn",
    "outputId": "30a624dd-ffbf-4678-96ba-d07f1a4627d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened webpage..\n",
      "Logged in....\n",
      "Got the code.....\n"
     ]
    }
   ],
   "source": [
    "CLIENT_ID = \"##########\"\n",
    "CLIENT_SECRET = \"*********\"\n",
    "REDIRECT_URI = \"http://localhost:8080\"\n",
    "\n",
    "r = RedditAPI(CLIENT_ID=CLIENT_ID, CLIENT_SECRET=CLIENT_SECRET, REDIRECT_URI=REDIRECT_URI)\n",
    "auth_token = r.get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSgNtTvEDwUs"
   },
   "outputs": [],
   "source": [
    "auth_headers = {\"Authorization\": f\"bearer {auth_token}\", \"User-Agent\": \"Mozilla/5.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i45jy2DVDwUy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'bearer 347754727638-_vZbh2JhEtbTqdCLTYPLiJrsSlk',\n",
       " 'User-Agent': 'Mozilla/5.0'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auth_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'q' : 'india', 'limit' : '10', 'sort' : 'hot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': 'india', 'limit': '10', 'sort': 'hot'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://oauth.reddit.com/r/subreddit/search\", headers=auth_headers, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a nested list containing first item as title and second as flair\n",
    "data_items = []\n",
    "# titles, flairs = [], []\n",
    "for i in range(10):\n",
    "    data_items.append([response.json()['data']['children'][i]['data']['title'], response.json()['data']['children'][i]['data']['link_flair_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data_items, columns=['title', 'flair'])  # converting above created list into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientists in India discovered a new snake spe...</td>\n",
       "      <td>Animal Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ronaldo visits India during lockdown</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Idea of India is too great for little mind...</td>\n",
       "      <td>OC है बे!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Never in my life I could have imagined that in...</td>\n",
       "      <td>Virat Hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Jatayu Earth center in kerala, India. Worl...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bar Dancer in India</td>\n",
       "      <td>Butthurt OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pakistani Twitter account posing as Omani prin...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Scientists in India discoverd a new snake spec...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stranded Oil Tankers around the world (Credit:...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Finally calling China’s bluff: India’s decisio...</td>\n",
       "      <td>Policy/Economy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           flair\n",
       "0  Scientists in India discovered a new snake spe...  Animal Science\n",
       "1               Ronaldo visits India during lockdown            None\n",
       "2  The Idea of India is too great for little mind...       OC है बे!\n",
       "3  Never in my life I could have imagined that in...     Virat Hindu\n",
       "4  The Jatayu Earth center in kerala, India. Worl...            None\n",
       "5                                Bar Dancer in India     Butthurt OP\n",
       "6  Pakistani Twitter account posing as Omani prin...        Politics\n",
       "7  Scientists in India discoverd a new snake spec...            News\n",
       "8  Stranded Oil Tankers around the world (Credit:...            None\n",
       "9  Finally calling China’s bluff: India’s decisio...  Policy/Economy"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AS WE CAN SEE THERE ARE LOTS OF HERE AND THERE ERRORS AND OTHER THINGS WE NEED TO TAKE CARE WHEN MAKING OAuth REQUESTS, SO WE ARE GOING TO USE NOW A SIMPLE TO USE WRAPPER - PRAW (PYTHON REDDIT API WRAPPER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-evUF9rDwU0"
   },
   "source": [
    "## METHOD 2 : Using standard API Wrappper - PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOH0SoPFDwU1"
   },
   "outputs": [],
   "source": [
    "# using packages\n",
    "import praw\n",
    "from praw.models import MoreComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "En4HoAY3DwU8"
   },
   "outputs": [],
   "source": [
    "class RedditParse:\n",
    "    \"\"\" \n",
    "    Class for implementing METHOD 2 , using PRAW WRAPPER\n",
    "      \n",
    "    Attributes: \n",
    "        url (str): the subreddit url  \n",
    "        headers (dict): headers containing user agent, and other optional args as part of requests\n",
    "        CLIENT_ID (str): client id taken from reddit app\n",
    "        CLIENT_SECRET (str) : client secret taken from reddit app\n",
    "        REDIRECT_URI (str) : client redirect url taken from reddit app\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, url, headers, CLIENT_ID, CLIENT_SECRET, REDIRECT_URI):\n",
    "        \"\"\" \n",
    "        Does the initialisation part for reddit requests\n",
    "\n",
    "        Parameters: \n",
    "            url (str): the subreddit url  \n",
    "            headers (dict): headers containing user agent, and other optional args as part of requests\n",
    "            CLIENT_ID (str): client id taken from reddit app\n",
    "            CLIENT_SECRET (str) : client secret taken from reddit app\n",
    "            REDIRECT_URI (str) : client redirect url taken from reddit app\n",
    "\n",
    "        Returns: \n",
    "            None\n",
    "  \n",
    "        \"\"\"\n",
    "        self.url = url \n",
    "        self.headers = headers\n",
    "        self.CLIENT_ID = CLIENT_ID\n",
    "        self.CLIENT_SECRET = CLIENT_SECRET\n",
    "        self.REDIRECT_URI = REDIRECT_URI\n",
    "            \n",
    "    def get_subreddit_with_flair(self, limit, topic, category, dict_ids):\n",
    "        \"\"\" \n",
    "        Does fetching of subreddit detailed info in json form\n",
    "\n",
    "        Parameters: \n",
    "            limit (int) : maximum number of results to return \n",
    "            topic (str) : subreddit topic to query for\n",
    "            category (str) : 'hot' or 'new'\n",
    "            dict_ids (dict) : dict empty when there is no existing dict_ids , or already existing dict_ids containing keys as \n",
    "                            id of returned result and values as frequency of how many time that id (result) already exists.\n",
    "\n",
    "        Returns: \n",
    "            data (dataframe): data is dataframe returned containing following items as columns : \n",
    "                            ['id', 'name', 'title', 'score', 'subreddit', 'url', 'num_comments', 'selftext', 'author', \n",
    "                            'num_crossposts', 'over_18', 'permalink', 'pinned', 'subreddit_type', 'link_flair_text', \n",
    "                            'author_flair_text', 'total_awards_received', 'upvote_ratio', 'time_created', 'comment']\n",
    "                \n",
    "            dict_ids (dict): dict_ids containing keys as \n",
    "                            id of returned result and values as frequency of how many time that id (result) already exists.\n",
    "  \n",
    "        \"\"\"\n",
    "        data_items = []\n",
    "        if dict_ids == None:\n",
    "            dict_ids = {}\n",
    "            print(\"Dict_ids empty, created new...\")\n",
    "        else:\n",
    "            dict_ids = dict_ids\n",
    "            print(\"Found already dict_ids created, using it..\")\n",
    "            \n",
    "        reddit = praw.Reddit(client_id=self.CLIENT_ID, client_secret=self.CLIENT_SECRET, user_agent=self.headers)\n",
    "        print(\"Fetching subreddit flair info...\")\n",
    "        if category == 'hot':\n",
    "            top_posts = reddit.subreddit(topic).hot(limit=limit)\n",
    "        elif category == 'new':\n",
    "            top_posts = reddit.subreddit(topic).new(limit=limit)\n",
    "            \n",
    "            \n",
    "        with tqdm(total=limit, position=0, leave=True) as pbar:\n",
    "            # lopping over all returned posts for a topic on reddit \n",
    "            for post in top_posts:\n",
    "                # lopping for every thread for every post\n",
    "                comment = \" \"\n",
    "                for top_level_comment in post.comments:\n",
    "                    if isinstance(top_level_comment, MoreComments):\n",
    "                        continue\n",
    "                    comment = comment + \" \" + top_level_comment.body\n",
    "                \n",
    "                # checking if id of returned result already existed, if not then take this \n",
    "                if post.id not in dict_ids:\n",
    "                    dict_ids[post.id] = 1\n",
    "                    data_items.append([post.id, post.name, post.title, post.score, post.subreddit, \n",
    "                                       post.url, post.num_comments, post.selftext, post.author,\n",
    "                                       post.num_crossposts, post.over_18, post.permalink, post.pinned, post.subreddit_type, \n",
    "                                       post.link_flair_text, post.author_flair_text, post.total_awards_received, post.upvote_ratio, \n",
    "                                       datetime.datetime.fromtimestamp(int(post.created_utc)).strftime('%Y-%m-%d %H:%M:%S'), comment[:1000]])\n",
    "                else:\n",
    "                    dict_ids[post.id] += 1\n",
    "                    \n",
    "\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        print(\"Building Dataframe...\")\n",
    "        data = pd.DataFrame(data_items, columns=['id', 'name', 'title', 'score', 'subreddit', 'url', 'num_comments', 'selftext', 'author', \n",
    "                                                'num_crossposts', 'over_18', 'permalink', 'pinned', 'subreddit_type', 'link_flair_text', \n",
    "                                                'author_flair_text', 'total_awards_received', 'upvote_ratio', 'time_created', 'comment'])        \n",
    "        return data, dict_ids\n",
    "    \n",
    "    def build_dataframe(self, limit, output_path, topic='all', category='hot', dict_ids=None):\n",
    "        \"\"\" \n",
    "        Does helps in building and saving the output file from dataframe fetched from get_subreddit_with_flair method. \n",
    "\n",
    "        Parameters: \n",
    "            limit (int) : maximum number of results to return \n",
    "            topic (str) : subreddit topic to query for\n",
    "            category (str) : 'hot' or 'new'\n",
    "            dict_ids (dict) : dict empty when there is no existing dict_ids , or already existing dict_ids containing keys as \n",
    "                            id of returned result and values as frequency of how many time that id (result) already exists.\n",
    "\n",
    "        Returns: \n",
    "            data (dataframe): \n",
    "            dict_ids (dict): \n",
    "  \n",
    "        \"\"\"\n",
    "        \n",
    "        data, dict_ids = self.get_subreddit_with_flair(limit, topic, category, dict_ids)\n",
    "        if os.path.isfile(output_path):\n",
    "            os.remove(output_path)\n",
    "            data.to_csv(output_path, index=False)\n",
    "            print(f\"File written to {output_path}\")\n",
    "            return dict_ids\n",
    "        else:\n",
    "            data.to_csv(output_path, index=False)\n",
    "            print(f\"File written to {output_path}\")\n",
    "            return dict_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mL1u_y1uDwVA"
   },
   "outputs": [],
   "source": [
    "url = \"https://www.reddit.com/r/india/\"\n",
    "headers = headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "CLIENT_ID = \"#########\"\n",
    "CLIENT_SECRET = \"***************\"\n",
    "REDIRECT_URI = \"http://localhost:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING 'NEW' DATA\n",
      "Dict_ids empty, created new...\n",
      "Fetching subreddit flair info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████████████████████████████████████▍          | 866/1000 [11:42<01:48,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Dataframe...\n",
      "File written to C:/Users/DELL/Desktop/reddit_data_classifier_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r = RedditParse(url, headers, CLIENT_ID, CLIENT_SECRET, REDIRECT_URI)\n",
    "print(\"BUILDING 'NEW' DATA\")\n",
    "dict_ids = r.build_dataframe(1000, 'C:/Users/DELL/Desktop/reddit_data_classifier_new.csv', 'india', 'new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING 'HOT' DATA\n",
      "Found already dict_ids created, using it..\n",
      "Fetching subreddit flair info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████████████████████████████▏                 | 774/1000 [10:48<03:09,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Dataframe...\n",
      "File written to C:/Users/DELL/Desktop/reddit_data_classifier_hot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"BUILDING 'HOT' DATA\")\n",
    "dict_ids = r.build_dataframe(1000, 'C:/Users/DELL/Desktop/reddit_data_classifier_hot.csv', 'india', 'hot', dict_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING 'HOT' DATA\n",
      "Dict_ids empty, created new...\n",
      "Fetching subreddit flair info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████▍                 | 777/1000 [12:04<03:27,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Dataframe...\n",
      "File written to C:/Users/DELL/Desktop/reddit_data_classifier_hot_mod.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# again downloading hot data for individual analyisation\n",
    "print(\"BUILDING 'HOT' DATA\")\n",
    "dict_ids_hot_new = r.build_dataframe(1000, 'C:/Users/DELL/Desktop/reddit_data_classifier_hot_mod.csv', 'india', 'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reddit_data_classifier_hot.csv', 'reddit_data_classifier_new.csv']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884, 20)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv.to_csv( \"C:/Users/DELL/Desktop/reddit_india_classifier_combined_data.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'UTF-8-SIG', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "with open('reddit_india_classifier_combined_data.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 3 : PUSHSHIFT API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLECTING MONTH WISE REDDIT INDIA DATA TITLE + FLAIRS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def getPushshiftData(after, before, sub):\n",
    "    \"\"\" \n",
    "        function for implementing METHOD 3, Helps in fetching reddit API using PushShift API\n",
    "\n",
    "        Parameters: \n",
    "            after (str) : results after which data is to be fetched (epoch UNIX time)\n",
    "            before (str) : results before which data is to be fetched (epoch UNIX time)\n",
    "            sub (str) : this is the name of subreddit\n",
    "\n",
    "        Returns: \n",
    "            data (dict) : json item containing various parameters (info) about query searched \n",
    "  \n",
    "        \"\"\"\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['id', 'title', 'author', 'author_flair_text', 'created_utc', 'full_link', 'num_comments', 'num_crossposts', 'over_18', 'permalink', 'url', 'total_awards_received', 'pinned', 'score', 'flair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?title=&size=1000&after=1577882040&before=1580474040&subreddit=india\n",
      "https://api.pushshift.io/reddit/search/submission/?title=&size=1000&after=1580560440&before=1582979640&subreddit=india\n",
      "https://api.pushshift.io/reddit/search/submission/?title=&size=1000&after=1583066040&before=1585658040&subreddit=india\n",
      "https://api.pushshift.io/reddit/search/submission/?title=&size=1000&after=1585744440&before=1588250040&subreddit=india\n"
     ]
    }
   ],
   "source": [
    "full_data_jan = getPushshiftData(1577882040, 1580474040, 'india')\n",
    "full_data_feb = getPushshiftData(1580560440, 1582979640, 'india')\n",
    "full_data_mar = getPushshiftData(1583066040, 1585658040, 'india')\n",
    "full_data_apr = getPushshiftData(1585744440, 1588250040, 'india')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_jan[910]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(full_data):\n",
    "    \"\"\" \n",
    "        function for getting nested lists of containing all parameters as features (columns) for dataset build dataframe\n",
    "\n",
    "        Parameters: \n",
    "            full_data (json) : json data fetched from PushShift API\n",
    "\n",
    "        Returns: \n",
    "            data_items (2-D list) : nested list , contaning 15 columns values named as [id, title, author, author_flair_text, \n",
    "            created_utc, full_link, num_comments, num_crossposts, over_18, permalink, url, total_awards_received, pinned, score, link_flair_text]\n",
    "  \n",
    "     \"\"\"\n",
    "    \n",
    "    data_items =  []\n",
    "\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            data_items.append([full_data[i]['id'], full_data[i]['title'], full_data[i]['author'], full_data[i]['author_flair_text'], \n",
    "                              full_data[i]['created_utc'], full_data[i]['full_link'], full_data[i]['num_comments'], full_data[i]['num_crossposts'], \n",
    "                               full_data[i]['over_18'], \n",
    "                              full_data[i]['permalink'], full_data[i]['url'], full_data[i]['total_awards_received'], full_data[i]['pinned'], full_data[i]['score'],\n",
    "                               full_data[i]['link_flair_text']])\n",
    "            \n",
    "        except Exception as error:\n",
    "            pass\n",
    "    \n",
    "    return data_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan, df_feb, df_mar, df_apr = pd.DataFrame(get_df(full_data_jan), columns=columns), pd.DataFrame(get_df(full_data_feb), columns=columns), pd.DataFrame(get_df(full_data_mar), columns=columns), pd.DataFrame(get_df(full_data_apr), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing four months data to disk\n",
    "df_jan.to_csv( \"C:/Users/DELL/Desktop/reddit_india_jan_data.csv\", index=False)\n",
    "df_feb.to_csv( \"C:/Users/DELL/Desktop/reddit_india_feb_data.csv\", index=False)\n",
    "df_mar.to_csv( \"C:/Users/DELL/Desktop/reddit_india_mar_data.csv\", index=False)\n",
    "df_apr.to_csv( \"C:/Users/DELL/Desktop/reddit_india_apr_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flairs(after, before, sub, flair):\n",
    "    \"\"\" \n",
    "        function for getting data items containing info on passed subreddits topic , mainly based on flair type\n",
    "\n",
    "        Parameters: \n",
    "            after (str) : results after which data is to be fetched (epoch UNIX time)\n",
    "            before (str) : results before which data is to be fetched (epoch UNIX time)\n",
    "            sub (str) : this is the name of subreddit\n",
    "            flair (str) : passed flair topics on which basis returned results will depend on\n",
    "\n",
    "        Returns: \n",
    "            data (json) : json feteched data from PushShift API\n",
    "  \n",
    "     \"\"\"\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)+'&link_flair_text='+str(flair)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    " \n",
    "def append_list_as_row(file_name, list_of_elem):\n",
    "    \"\"\" \n",
    "        function for writing a new row to the end of csv\n",
    "        \n",
    "        Parameters: \n",
    "            file_name (str) : file on which writing operation is to be done\n",
    "            list_of_elem (list) : list of all column values of dataframe\n",
    "\n",
    "        Returns: \n",
    "            data (json) : json feteched data from PushShift API\n",
    "  \n",
    "     \"\"\"\n",
    "    \n",
    "    # Open file in append mode\n",
    "    with open(file_name, 'a+', newline='') as write_obj:\n",
    "        # Create a writer object from csv module\n",
    "        csv_writer = writer(write_obj)\n",
    "        # Add contents of list as last row in the csv file\n",
    "        csv_writer.writerow(list_of_elem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3074, 15)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_filenames = ['reddit_india_jan_data.csv', 'reddit_india_feb_data.csv', 'reddit_india_mar_data.csv', 'reddit_india_apr_data.csv']\n",
    "combined_csv_new = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv_new.to_csv( \"C:/Users/DELL/Desktop/reddit_india_classifier_combined_data.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?title=&size=1000&after=1585744440&before=1588250040&subreddit=india\n"
     ]
    }
   ],
   "source": [
    "full_data_april = getPushshiftData(1585744440, 1588250040, 'india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_ids = [full_data_april[i]['id'] for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fsyy5x'"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apr_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FETCHING ALL COMMENTS DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(after, before, sub):\n",
    "    \"\"\" \n",
    "        function for getting detailed info about commments based on specific subreddit topic , on time interval\n",
    "        \n",
    "        Parameters: \n",
    "            after (str) : results after which data is to be fetched (epoch UNIX time)\n",
    "            before (str) : results before which data is to be fetched (epoch UNIX time)\n",
    "            sub (str) : subreddit topic to search for \n",
    "\n",
    "        Returns: \n",
    "            data (json) : json feteched data from PushShift API, all comments data\n",
    "  \n",
    "     \"\"\"\n",
    "    url = 'https://api.pushshift.io/reddit/search/comment/?subreddit='+str(sub)+'&size=1000&after='+str(after)+'&before='+str(before)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/comment/?subreddit=india&size=1000&after=1577882040&before=1580474040\n",
      "https://api.pushshift.io/reddit/search/comment/?subreddit=india&size=1000&after=1580560440&before=1582979640\n",
      "https://api.pushshift.io/reddit/search/comment/?subreddit=india&size=1000&after=1583066040&before=1585658040\n",
      "https://api.pushshift.io/reddit/search/comment/?subreddit=india&size=1000&after=1585744440&before=1588250040\n"
     ]
    }
   ],
   "source": [
    "comments_jan = get_comments(1577882040, 1580474040, 'india')\n",
    "comments_feb = get_comments(1580560440, 1582979640, 'india')\n",
    "comments_mar = get_comments(1583066040, 1585658040, 'india')\n",
    "comments_apr = get_comments(1585744440, 1588250040, 'india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list = ['id', 'parent_id', 'link_id', 'body', 'author', 'author_flair_text', 'author_fullname', 'permalink', 'created_utc', 'score', 'total_awards_received']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_month(comment_data):\n",
    "    \"\"\" \n",
    "        function for getting nested lists of containing all parameters as features (columns) for comments dataset build dataframe\n",
    "        \n",
    "        Parameters: \n",
    "            comment_data (json) : full json params fetched from PushShift API\n",
    "\n",
    "        Returns: \n",
    "            data_items (2-D list) : nested list , contaning 11 columns values named as [id, parent_id, link_id, body, \n",
    "            author, author_flair_text, author_fullname, permalink, created_utc, score, url, total_awards_received]\n",
    "  \n",
    "     \"\"\"\n",
    "    data_items =  []\n",
    "\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            data_items.append([comment_data[i]['id'], comment_data[i]['parent_id'], comment_data[i]['link_id'], comment_data[i]['body'], comment_data[i]['author'],\n",
    "                              comment_data[i]['author_flair_text'], comment_data[i]['author_fullname'], comment_data[i]['permalink'], \n",
    "                               comment_data[i]['created_utc'], comment_data[i]['score'], comment_data[i]['total_awards_received']])\n",
    "            \n",
    "        except Exception as error:\n",
    "            pass\n",
    "    \n",
    "    return data_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_jan, com_feb, com_mar, com_apr = pd.DataFrame(get_comment_month(comments_jan), columns=comments_list), pd.DataFrame(get_comment_month(comments_feb), columns=comments_list), pd.DataFrame(get_comment_month(comments_mar), columns=comments_list), pd.DataFrame(get_comment_month(comments_apr), columns=comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing four months data comments to disk\n",
    "com_jan.to_csv( \"C:/Users/DELL/Desktop/reddit_india_jan_data_com.csv\", index=False)\n",
    "com_feb.to_csv( \"C:/Users/DELL/Desktop/reddit_india_feb_data_com.csv\", index=False)\n",
    "com_mar.to_csv( \"C:/Users/DELL/Desktop/reddit_india_mar_data_com.csv\", index=False)\n",
    "com_apr.to_csv( \"C:/Users/DELL/Desktop/reddit_india_apr_data_com.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3885, 11)"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_filenames_comments = ['reddit_india_jan_data_com.csv', 'reddit_india_feb_data_com.csv', 'reddit_india_mar_data_com.csv', 'reddit_india_apr_data_com.csv']\n",
    "combined_csv_comments = pd.concat([pd.read_csv(f) for f in all_filenames_comments])\n",
    "combined_csv_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the combined dataset for four months \n",
    "combined_csv_comments.to_csv( \"C:/Users/DELL/Desktop/reddit_india_comments_data.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "REDDIT_DATA_EXPLORE_AND_CLASSIFY_FLAIRS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
